// 1. Récupérer le fichier CSV depuis GitHub
def gitClone() {
    git clone "https://github.com/Jh2David/analyse-de-sentiments"
    csv_file = "data/tweets.csv"
}

// 2. Vérifier si le fichier CSV a été modifié depuis la dernière exécution
def checkCSVModified() {
    if (sh(script: "git diff --quiet $csv_file", returnStatus: true) == 0) {
        echo "Aucune modification du fichier CSV détectée. Arrêt du script."
        return true
    }
    return false
}

// 3. Vérifier que le fichier CSV n'est pas vide
def checkCSVNotEmpty() {
    if (sh(script: "[ ! -s \"$csv_file\" ]", returnStatus: true) == 0) {
        echo "Le fichier CSV est vide. Impossible de continuer."
        return false
    }
    return true
}

// 4. Transférer le fichier CSV vers Azure Databricks
def uploadToDatabricks() {
    sh "dbfs cp $csv_file dbfs:/data/tweets.csv"
}

// 5. Exécuter les notebooks 2 et 3 sur Azure Databricks
def notebookPath2 = "/Workspace/Users/jehan@lepont-learning.com/2 - Nettoyage et prétraitement des données"


def runNotebooks() {
sh "databricks runs submit notebook --notebook-path '$notebookPath2' --params '{}'"
}

pipeline {
    agent any
    environment {
        DATABRICKS_HOST = credentials('databricks-host')
        DATABRICKS_CLIENT_ID = credentials('databricks-client-id')
        DATABRICKS_CLIENT_SECRET = credentials('databricks-client-secret')
    }
    stages {
        stage('Prepare Environment') {
            steps {
                script {
                    gitClone()
                    if (!checkCSVModified() && checkCSVNotEmpty()) {
                        uploadToDatabricks()
                        runNotebooks()
                    }
                }
            }
        }
    }
}
